{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel Booking Analysis: Advanced Analytics and Business Impact Study\n",
    "\n",
    "## Project Overview\n",
    "This analysis explores hotel booking patterns and cancellation behaviors to optimize revenue and operational efficiency. Using advanced analytics and machine learning techniques, we aim to provide actionable insights for strategic decision-making.\n",
    "\n",
    "## Business Objectives\n",
    "1. Identify key factors driving booking cancellations\n",
    "2. Develop customer segmentation profiles\n",
    "3. Quantify revenue impact of cancellations\n",
    "4. Create predictive models for early cancellation detection\n",
    "5. Propose data-driven strategies for revenue optimization\n",
    "\n",
    "## Methodology\n",
    "1. Data Quality Assessment and Preprocessing\n",
    "2. Advanced Feature Engineering\n",
    "3. Statistical Analysis and Hypothesis Testing\n",
    "4. Customer Segmentation Analysis\n",
    "5. Predictive Modeling\n",
    "6. Business Impact Evaluation\n",
    "\n",
    "## Expected Deliverables\n",
    "- Comprehensive cancellation risk profiles\n",
    "- Revenue impact analysis by segment\n",
    "- ML model for cancellation prediction\n",
    "- Actionable recommendations for management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, roc_auc_score\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Visualization Settings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# 1. Environment Setup and Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Visualization Settings\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('hotel_analysis.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress Warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Create Output Directories\n",
    "output_dirs = ['outputs/visuals', 'outputs/models', 'outputs/reports']\n",
    "for dir_path in output_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "logger.info(\"Environment setup completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Loading and Quality Assessment\n",
    "def load_and_validate_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and perform initial validation of the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the hotel bookings dataset\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Validated and initially processed dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        logger.info(f\"Loading dataset from {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Initial data quality checks\n",
    "        logger.info(\"\\nPerforming initial data quality checks:\")\n",
    "        logger.info(f\"Shape of dataset: {df.shape}\")\n",
    "        logger.info(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "        logger.info(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "        \n",
    "        # Data type validation\n",
    "        logger.info(\"\\nData types of columns:\")\n",
    "        logger.info(df.dtypes)\n",
    "        \n",
    "        # Basic statistics\n",
    "        logger.info(\"\\nBasic statistics of numerical columns:\")\n",
    "        logger.info(df.describe())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load the dataset\n",
    "data = load_and_validate_data('data/raw/hotel_bookings.csv')\n",
    "\n",
    "# Display sample of the data\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Cleaning\n",
    "\n",
    "### Objectives:\n",
    "1. Handle missing values appropriately\n",
    "2. Remove duplicates and anomalies\n",
    "3. Correct data types and formats\n",
    "4. Handle outliers\n",
    "5. Ensure data consistency\n",
    "\n",
    "### Approach:\n",
    "- Use domain knowledge for missing value imputation\n",
    "- Implement robust outlier detection\n",
    "- Document all cleaning decisions for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Data Cleaning Functions\n",
    "def analyze_missing_values(df):\n",
    "    \"\"\"\n",
    "    Analyze missing values and their patterns.\n",
    "    \"\"\"\n",
    "    # Calculate missing value statistics\n",
    "    missing_stats = pd.DataFrame({\n",
    "        'Missing_Count': df.isnull().sum(),\n",
    "        'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    }).sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    logger.info(\"\\nMissing Value Analysis:\")\n",
    "    logger.info(missing_stats)\n",
    "    \n",
    "    return missing_stats\n",
    "\n",
    "def detect_outliers(df, columns):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method.\n",
    "    \"\"\"\n",
    "    outliers_dict = {}\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]\n",
    "        outliers_dict[column] = len(outliers)\n",
    "    \n",
    "    logger.info(\"\\nOutlier Detection Results:\")\n",
    "    logger.info(outliers_dict)\n",
    "    \n",
    "    return outliers_dict\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handle missing values using appropriate strategies.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Children: Fill with 0 (assumption: missing means no children)\n",
    "    df_clean['children'] = df_clean['children'].fillna(0)\n",
    "    \n",
    "    # Country: Fill with 'Unknown'\n",
    "    df_clean['country'] = df_clean['country'].fillna('Unknown')\n",
    "    \n",
    "    # Agent: Fill with 0 (direct booking)\n",
    "    df_clean['agent'] = df_clean['agent'].fillna(0)\n",
    "    \n",
    "    # Company: Fill with 0 (no company association)\n",
    "    df_clean['company'] = df_clean['company'].fillna(0)\n",
    "    \n",
    "    logger.info(\"\\nMissing values handled successfully\")\n",
    "    return df_clean\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Main data cleaning function.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting data cleaning process...\")\n",
    "        \n",
    "        # 1. Analyze missing values\n",
    "        missing_stats = analyze_missing_values(df)\n",
    "        \n",
    "        # 2. Handle missing values\n",
    "        df_clean = handle_missing_values(df)\n",
    "        \n",
    "        # 3. Remove duplicates\n",
    "        duplicates = df_clean.duplicated().sum()\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "        logger.info(f\"\\nRemoved {duplicates} duplicate rows\")\n",
    "        \n",
    "        # 4. Detect outliers in numerical columns\n",
    "        numerical_cols = df_clean.select_dtypes(include=['float64', 'int64']).columns\n",
    "        outliers = detect_outliers(df_clean, numerical_cols)\n",
    "        \n",
    "        # 5. Convert data types\n",
    "        df_clean['arrival_date'] = pd.to_datetime(\n",
    "            df_clean['arrival_date_year'].astype(str) + '-' +\n",
    "            df_clean['arrival_date_month'] + '-' +\n",
    "            df_clean['arrival_date_day_of_month'].astype(str)\n",
    "        )\n",
    "        \n",
    "        # 6. Drop unnecessary columns\n",
    "        cols_to_drop = ['arrival_date_year', 'arrival_date_month', 'arrival_date_day_of_month']\n",
    "        df_clean = df_clean.drop(columns=cols_to_drop)\n",
    "        \n",
    "        logger.info(\"Data cleaning completed successfully\")\n",
    "        return df_clean\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in data cleaning: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Apply data cleaning\n",
    "data_clean = clean_data(data)\n",
    "\n",
    "# Display cleaning results\n",
    "print(\"\\nDataset shape after cleaning:\", data_clean.shape)\n",
    "print(\"\\nData types after cleaning:\")\n",
    "print(data_clean.dtypes)\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "display(data_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Data Quality Validation\n",
    "def validate_data_quality(df):\n",
    "    \"\"\"\n",
    "    Validate the quality of cleaned data.\n",
    "    \"\"\"\n",
    "    validation_results = {\n",
    "        'missing_values': df.isnull().sum().sum(),\n",
    "        'duplicates': df.duplicated().sum(),\n",
    "        'negative_values': {\n",
    "            col: (df[col] < 0).sum() \n",
    "            for col in df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check date ranges\n",
    "    validation_results['date_range'] = {\n",
    "        'min_date': df['arrival_date'].min(),\n",
    "        'max_date': df['arrival_date'].max()\n",
    "    }\n",
    "    \n",
    "    # Validate categorical variables\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    validation_results['categorical_counts'] = {\n",
    "        col: df[col].nunique() \n",
    "        for col in categorical_cols\n",
    "    }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Perform validation\n",
    "validation_results = validate_data_quality(data_clean)\n",
    "\n",
    "# Display validation results\n",
    "print(\"\\nData Quality Validation Results:\")\n",
    "print(\"--------------------------------\")\n",
    "print(f\"Missing Values: {validation_results['missing_values']}\")\n",
    "print(f\"Duplicates: {validation_results['duplicates']}\")\n",
    "print(\"\\nNegative Values Check:\")\n",
    "for col, count in validation_results['negative_values'].items():\n",
    "    if count > 0:\n",
    "        print(f\"- {col}: {count} negative values\")\n",
    "print(\"\\nDate Range:\")\n",
    "print(f\"- From: {validation_results['date_range']['min_date']}\")\n",
    "print(f\"- To: {validation_results['date_range']['max_date']}\")\n",
    "print(\"\\nCategorical Variable Counts:\")\n",
    "for col, count in validation_results['categorical_counts'].items():\n",
    "    print(f\"- {col}: {count} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Transformation\n",
    "\n",
    "### Objectives:\n",
    "1. Create meaningful temporal features\n",
    "2. Generate price-related metrics\n",
    "3. Develop guest composition features\n",
    "4. Engineer market segment indicators\n",
    "5. Create interaction features\n",
    "\n",
    "### Approach:\n",
    "- Focus on business-relevant feature creation\n",
    "- Ensure features are interpretable\n",
    "- Document feature importance and rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Feature Engineering Functions\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    A class to handle all feature engineering operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def create_temporal_features(self):\n",
    "        \"\"\"Create time-based features\"\"\"\n",
    "        self.df['booking_year'] = self.df['arrival_date'].dt.year\n",
    "        self.df['booking_month'] = self.df['arrival_date'].dt.month\n",
    "        self.df['booking_day'] = self.df['arrival_date'].dt.day\n",
    "        self.df['booking_dayofweek'] = self.df['arrival_date'].dt.dayofweek\n",
    "        self.df['is_weekend_arrival'] = self.df['booking_dayofweek'].isin([5, 6]).astype(int)\n",
    "        self.df['is_peak_season'] = self.df['booking_month'].isin([7, 8, 12]).astype(int)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_price_features(self):\n",
    "        \"\"\"Create price-related features\"\"\"\n",
    "        self.df['total_nights'] = self.df['stays_in_weekend_nights'] + self.df['stays_in_week_nights']\n",
    "        self.df['total_cost'] = self.df['adr'] * self.df['total_nights']\n",
    "        self.df['avg_price_per_person'] = self.df['total_cost'] / (self.df['adults'] + self.df['children'] + self.df['babies'])\n",
    "        self.df['is_high_price'] = (self.df['adr'] > self.df['adr'].mean()).astype(int)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_guest_features(self):\n",
    "        \"\"\"Create guest composition features\"\"\"\n",
    "        self.df['total_guests'] = self.df['adults'] + self.df['children'] + self.df['babies']\n",
    "        self.df['has_children'] = ((self.df['children'] > 0) | (self.df['babies'] > 0)).astype(int)\n",
    "        self.df['is_family'] = ((self.df['adults'] >= 2) & (self.df['has_children'] == 1)).astype(int)\n",
    "        self.df['is_single'] = ((self.df['adults'] == 1) & (self.df['has_children'] == 0)).astype(int)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_booking_features(self):\n",
    "        \"\"\"Create booking-related features\"\"\"\n",
    "        self.df['is_long_stay'] = (self.df['total_nights'] > 7).astype(int)\n",
    "        self.df['is_long_lead_time'] = (self.df['lead_time'] > 90).astype(int)\n",
    "        self.df['has_special_requests'] = (self.df['total_of_special_requests'] > 0).astype(int)\n",
    "        self.df['is_repeated_guest'] = self.df['is_repeated_guest'].astype(int)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_market_features(self):\n",
    "        \"\"\"Create market segment features\"\"\"\n",
    "        self.df['is_direct_booking'] = (self.df['market_segment'] == 'Direct').astype(int)\n",
    "        self.df['is_corporate'] = (self.df['market_segment'] == 'Corporate').astype(int)\n",
    "        \n",
    "        # One-hot encode market segment and meal\n",
    "        market_dummies = pd.get_dummies(self.df['market_segment'], prefix='market')\n",
    "        meal_dummies = pd.get_dummies(self.df['meal'], prefix='meal')\n",
    "        self.df = pd.concat([self.df, market_dummies, meal_dummies], axis=1)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform_all_features(self):\n",
    "        \"\"\"Apply all feature transformations\"\"\"\n",
    "        self.logger.info(\"Starting feature engineering process...\")\n",
    "        \n",
    "        (self.create_temporal_features()\n",
    "             .create_price_features()\n",
    "             .create_guest_features()\n",
    "             .create_booking_features()\n",
    "             .create_market_features())\n",
    "        \n",
    "        self.logger.info(\"Feature engineering completed successfully\")\n",
    "        return self.df\n",
    "\n",
    "# Apply feature engineering\n",
    "feature_engineer = FeatureEngineer(data_clean)\n",
    "data_featured = feature_engineer.transform_all_features()\n",
    "\n",
    "# Display new features summary\n",
    "print(\"\\nNew Features Created:\")\n",
    "print(\"--------------------\")\n",
    "new_features = set(data_featured.columns) - set(data_clean.columns)\n",
    "print(f\"Total new features created: {len(new_features)}\")\n",
    "print(\"\\nSample of new features:\")\n",
    "print(sorted(list(new_features))[:10])\n",
    "\n",
    "# Display sample of transformed data\n",
    "print(\"\\nSample of transformed data:\")\n",
    "display(data_featured.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis\n",
    "\n",
    "### Objectives:\n",
    "1. Understand distributions and relationships\n",
    "2. Test hypotheses about booking patterns\n",
    "3. Identify significant correlations\n",
    "4. Analyze seasonal patterns\n",
    "5. Evaluate cancellation factors\n",
    "\n",
    "### Approach:\n",
    "- Rigorous statistical testing\n",
    "- Clear visualization of results\n",
    "- Focus on business implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Statistical Analysis Functions\n",
    "class StatisticalAnalyzer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def run_hypothesis_tests(self):\n",
    "        \"\"\"Perform statistical hypothesis tests\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Test 1: Is there a significant difference in ADR between canceled and non-canceled bookings?\n",
    "        t_stat, p_value = stats.ttest_ind(\n",
    "            self.df[self.df['is_canceled'] == 1]['adr'],\n",
    "            self.df[self.df['is_canceled'] == 0]['adr']\n",
    "        )\n",
    "        results['adr_cancellation'] = {'t_statistic': t_stat, 'p_value': p_value}\n",
    "        \n",
    "        # Test 2: Chi-square test for independence between market segment and cancellation\n",
    "        contingency = pd.crosstab(self.df['market_segment'], self.df['is_canceled'])\n",
    "        chi2, p_value, dof, expected = stats.chi2_contingency(contingency)\n",
    "        results['market_cancellation'] = {'chi2': chi2, 'p_value': p_value}\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_correlations(self):\n",
    "        \"\"\"Analyze correlations between numerical variables\"\"\"\n",
    "        numerical_cols = self.df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        correlation_matrix = self.df[numerical_cols].corr()\n",
    "        \n",
    "        # Find strong correlations\n",
    "        strong_correlations = []\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(correlation_matrix.iloc[i, j]) > 0.5:\n",
    "                    strong_correlations.append({\n",
    "                        'var1': correlation_matrix.columns[i],\n",
    "                        'var2': correlation_matrix.columns[j],\n",
    "                        'correlation': correlation_matrix.iloc[i, j]\n",
    "                    })\n",
    "        \n",
    "        return correlation_matrix, strong_correlations\n",
    "    \n",
    "    def analyze_seasonality(self):\n",
    "        \"\"\"Analyze seasonal patterns\"\"\"\n",
    "        seasonal_stats = self.df.groupby('booking_month').agg({\n",
    "            'is_canceled': 'mean',\n",
    "            'adr': 'mean',\n",
    "            'total_guests': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        return seasonal_stats\n",
    "    \n",
    "    def perform_analysis(self):\n",
    "        \"\"\"Run all statistical analyses\"\"\"\n",
    "        self.logger.info(\"Starting statistical analysis...\")\n",
    "        \n",
    "        # 1. Hypothesis Tests\n",
    "        hypothesis_results = self.run_hypothesis_tests()\n",
    "        \n",
    "        # 2. Correlation Analysis\n",
    "        corr_matrix, strong_corrs = self.analyze_correlations()\n",
    "        \n",
    "        # 3. Seasonality Analysis\n",
    "        seasonal_patterns = self.analyze_seasonality()\n",
    "        \n",
    "        self.logger.info(\"Statistical analysis completed successfully\")\n",
    "        return hypothesis_results, corr_matrix, strong_corrs, seasonal_patterns\n",
    "\n",
    "# Perform statistical analysis\n",
    "analyzer = StatisticalAnalyzer(data_featured)\n",
    "hyp_results, corr_matrix, strong_corrs, seasonal_patterns = analyzer.perform_analysis()\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHypothesis Test Results:\")\n",
    "print(\"------------------------\")\n",
    "for test, results in hyp_results.items():\n",
    "    print(f\"\\n{test}:\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nStrong Correlations:\")\n",
    "print(\"-------------------\")\n",
    "for corr in strong_corrs[:5]:\n",
    "    print(f\"{corr['var1']} vs {corr['var2']}: {corr['correlation']:.2f}\")\n",
    "\n",
    "print(\"\\nSeasonal Patterns:\")\n",
    "print(\"----------------\")\n",
    "print(seasonal_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Analytics\n",
    "\n",
    "### Objectives:\n",
    "1. Customer Segmentation Analysis\n",
    "2. Pattern Recognition in Booking Behaviors\n",
    "3. Anomaly Detection\n",
    "4. Revenue Impact Analysis\n",
    "5. Risk Profiling\n",
    "\n",
    "### Approach:\n",
    "- Use clustering for customer segmentation\n",
    "- Implement pattern mining algorithms\n",
    "- Develop comprehensive risk profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Advanced Analytics Implementation\n",
    "class AdvancedAnalytics:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def perform_customer_segmentation(self, n_clusters=4):\n",
    "        \"\"\"\n",
    "        Perform customer segmentation using K-means clustering\n",
    "        \"\"\"\n",
    "        # Select features for clustering\n",
    "        cluster_features = [\n",
    "            'adr', 'total_nights', 'lead_time', 'total_guests',\n",
    "            'is_repeated_guest', 'previous_cancellations'\n",
    "        ]\n",
    "        \n",
    "        # Prepare data for clustering\n",
    "        X = self.scaler.fit_transform(self.df[cluster_features])\n",
    "        \n",
    "        # Perform K-means clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.df['customer_segment'] = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Analyze segments\n",
    "        segment_profiles = self.df.groupby('customer_segment').agg({\n",
    "            'adr': 'mean',\n",
    "            'total_nights': 'mean',\n",
    "            'lead_time': 'mean',\n",
    "            'total_guests': 'mean',\n",
    "            'is_canceled': 'mean',\n",
    "            'is_repeated_guest': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        return segment_profiles\n",
    "    \n",
    "    def detect_anomalies(self):\n",
    "        \"\"\"\n",
    "        Detect anomalous bookings using statistical methods\n",
    "        \"\"\"\n",
    "        anomalies = {\n",
    "            'price_anomalies': self.df[np.abs(stats.zscore(self.df['adr'])) > 3],\n",
    "            'stay_anomalies': self.df[np.abs(stats.zscore(self.df['total_nights'])) > 3],\n",
    "            'lead_time_anomalies': self.df[np.abs(stats.zscore(self.df['lead_time'])) > 3]\n",
    "        }\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def analyze_revenue_impact(self):\n",
    "        \"\"\"\n",
    "        Analyze revenue impact of different factors\n",
    "        \"\"\"\n",
    "        revenue_analysis = {\n",
    "            'total_revenue': (self.df['adr'] * self.df['total_nights']).sum(),\n",
    "            'lost_revenue': (self.df[self.df['is_canceled'] == 1]['adr'] * \n",
    "                           self.df[self.df['is_canceled'] == 1]['total_nights']).sum(),\n",
    "            'revenue_by_segment': self.df.groupby('market_segment').agg({\n",
    "                'adr': lambda x: (x * self.df['total_nights']).sum(),\n",
    "                'is_canceled': 'mean'\n",
    "            }).round(2)\n",
    "        }\n",
    "        \n",
    "        return revenue_analysis\n",
    "    \n",
    "    def create_risk_profiles(self):\n",
    "        \"\"\"\n",
    "        Create booking risk profiles\n",
    "        \"\"\"\n",
    "        # Calculate risk scores\n",
    "        self.df['risk_score'] = (\n",
    "            0.3 * (self.df['lead_time'] > 90).astype(int) +\n",
    "            0.2 * (self.df['adr'] > self.df['adr'].mean()).astype(int) +\n",
    "            0.2 * self.df['previous_cancellations'] +\n",
    "            0.3 * (self.df['total_nights'] > 7).astype(int)\n",
    "        )\n",
    "        \n",
    "        risk_profiles = self.df.groupby(pd.qcut(self.df['risk_score'], q=4)).agg({\n",
    "            'is_canceled': 'mean',\n",
    "            'adr': 'mean',\n",
    "            'lead_time': 'mean',\n",
    "            'total_nights': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        return risk_profiles\n",
    "    \n",
    "    def run_advanced_analytics(self):\n",
    "        \"\"\"\n",
    "        Run all advanced analytics\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting advanced analytics...\")\n",
    "        \n",
    "        # 1. Customer Segmentation\n",
    "        segment_profiles = self.perform_customer_segmentation()\n",
    "        \n",
    "        # 2. Anomaly Detection\n",
    "        anomalies = self.detect_anomalies()\n",
    "        \n",
    "        # 3. Revenue Impact Analysis\n",
    "        revenue_analysis = self.analyze_revenue_impact()\n",
    "        \n",
    "        # 4. Risk Profiling\n",
    "        risk_profiles = self.create_risk_profiles()\n",
    "        \n",
    "        self.logger.info(\"Advanced analytics completed successfully\")\n",
    "        return segment_profiles, anomalies, revenue_analysis, risk_profiles\n",
    "\n",
    "# Perform advanced analytics\n",
    "advanced_analyzer = AdvancedAnalytics(data_featured)\n",
    "segments, anomalies, revenue, risks = advanced_analyzer.run_advanced_analytics()\n",
    "\n",
    "# Display results\n",
    "print(\"Customer Segment Profiles:\")\n",
    "print(\"-----------------------\")\n",
    "print(segments)\n",
    "\n",
    "print(\"\\nRevenue Analysis:\")\n",
    "print(\"---------------\")\n",
    "print(f\"Total Revenue: ${revenue['total_revenue']:,.2f}\")\n",
    "print(f\"Lost Revenue from Cancellations: ${revenue['lost_revenue']:,.2f}\")\n",
    "\n",
    "print(\"\\nRisk Profiles:\")\n",
    "print(\"------------\")\n",
    "print(risks)\n",
    "\n",
    "# Visualize customer segments\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=data_featured,\n",
    "    x='adr',\n",
    "    y='lead_time',\n",
    "    hue='customer_segment',\n",
    "    palette='deep'\n",
    ")\n",
    "plt.title('Customer Segments by ADR and Lead Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predictive Modeling\n",
    "\n",
    "In this section, we'll develop and evaluate machine learning models to predict hotel booking cancellations. Our approach includes:\n",
    "\n",
    "1. Data Preparation\n",
    "   - Feature selection\n",
    "   - Train-test split\n",
    "   - Feature scaling\n",
    "   \n",
    "2. Model Development\n",
    "   - Random Forest Classifier (baseline)\n",
    "   - XGBoost\n",
    "   - LightGBM\n",
    "   \n",
    "3. Model Evaluation\n",
    "   - Cross-validation\n",
    "   - Hyperparameter tuning\n",
    "   - Performance metrics (accuracy, precision, recall, F1-score)\n",
    "   - ROC curves and AUC scores\n",
    "   \n",
    "4. Model Interpretation\n",
    "   - Feature importance analysis\n",
    "   - SHAP values for model explainability\n",
    "   - Partial dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import shap\n",
    "import logging\n",
    "\n",
    "class PredictiveModeling:\n",
    "    def __init__(self, data, target='is_canceled', test_size=0.2, random_state=42):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare data for modeling.\"\"\"\n",
    "        try:\n",
    "            # Select features (excluding target and unnecessary columns)\n",
    "            exclude_cols = ['is_canceled', 'reservation_status', 'reservation_status_date', 'ID']\n",
    "            feature_cols = [col for col in self.data.columns if col not in exclude_cols]\n",
    "            \n",
    "            # Convert categorical variables to dummy variables\n",
    "            X = pd.get_dummies(self.data[feature_cols], drop_first=True)\n",
    "            y = self.data[self.target]\n",
    "            \n",
    "            # Split data\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "                X, y, test_size=self.test_size, random_state=self.random_state\n",
    "            )\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            self.X_train_scaled = scaler.fit_transform(self.X_train)\n",
    "            self.X_test_scaled = scaler.transform(self.X_test)\n",
    "            \n",
    "            self.feature_names = X.columns\n",
    "            self.logger.info(f\"Data prepared successfully. Training set shape: {self.X_train.shape}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in data preparation: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# Initialize the modeling class\n",
    "modeling = PredictiveModeling(data_clean)\n",
    "modeling.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(modeling):\n",
    "    \"\"\"Train and evaluate multiple models.\"\"\"\n",
    "    # Random Forest\n",
    "    rf_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state=modeling.random_state)\n",
    "    rf_search = RandomizedSearchCV(rf, rf_params, n_iter=20, cv=5, random_state=modeling.random_state)\n",
    "    rf_search.fit(modeling.X_train_scaled, modeling.y_train)\n",
    "    modeling.models['random_forest'] = rf_search.best_estimator_\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_params = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'min_child_weight': [1, 3, 5]\n",
    "    }\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(random_state=modeling.random_state)\n",
    "    xgb_search = RandomizedSearchCV(xgb_model, xgb_params, n_iter=20, cv=5, random_state=modeling.random_state)\n",
    "    xgb_search.fit(modeling.X_train_scaled, modeling.y_train)\n",
    "    modeling.models['xgboost'] = xgb_search.best_estimator_\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_params = {\n",
    "        'num_leaves': [31, 50, 70],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'min_child_samples': [20, 30, 50]\n",
    "    }\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(random_state=modeling.random_state)\n",
    "    lgb_search = RandomizedSearchCV(lgb_model, lgb_params, n_iter=20, cv=5, random_state=modeling.random_state)\n",
    "    lgb_search.fit(modeling.X_train_scaled, modeling.y_train)\n",
    "    modeling.models['lightgbm'] = lgb_search.best_estimator_\n",
    "    \n",
    "    # Evaluate models\n",
    "    results = {}\n",
    "    for name, model in modeling.models.items():\n",
    "        y_pred = model.predict(modeling.X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(modeling.X_test_scaled)[:, 1]\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': accuracy_score(modeling.y_test, y_pred),\n",
    "            'precision': precision_score(modeling.y_test, y_pred),\n",
    "            'recall': recall_score(modeling.y_test, y_pred),\n",
    "            'f1': f1_score(modeling.y_test, y_pred),\n",
    "            'auc_roc': roc_auc_score(modeling.y_test, y_pred_proba),\n",
    "            'confusion_matrix': confusion_matrix(modeling.y_test, y_pred)\n",
    "        }\n",
    "    \n",
    "    modeling.results = results\n",
    "    return pd.DataFrame(results).round(3)\n",
    "\n",
    "# Train and evaluate models\n",
    "results_df = train_and_evaluate_models(modeling)\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_models(modeling):\n",
    "    \"\"\"Generate and visualize model interpretations.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Random Forest Feature Importance\n",
    "    rf_importance = pd.DataFrame({\n",
    "        'feature': modeling.feature_names,\n",
    "        'importance': modeling.models['random_forest'].feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.barplot(data=rf_importance, x='importance', y='feature')\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.xlabel('Importance Score')\n",
    "    \n",
    "    # SHAP Values for XGBoost\n",
    "    explainer = shap.TreeExplainer(modeling.models['xgboost'])\n",
    "    shap_values = explainer.shap_values(modeling.X_test_scaled)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    shap.summary_plot(shap_values, modeling.X_test, plot_type='bar', show=False)\n",
    "    plt.title('SHAP Feature Importance (XGBoost)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Generate partial dependence plots for top features\n",
    "    top_features = rf_importance['feature'].head(3).tolist()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    for i, feature in enumerate(top_features):\n",
    "        PartialDependenceDisplay.from_estimator(\n",
    "            modeling.models['random_forest'],\n",
    "            modeling.X_train,\n",
    "            [feature],\n",
    "            ax=axes[i]\n",
    "        )\n",
    "        axes[i].set_title(f'Partial Dependence Plot: {feature}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interpret models\n",
    "interpret_models(modeling)\n",
    "\n",
    "# Print best model insights\n",
    "best_model = max(modeling.results.items(), key=lambda x: x[1]['f1'])[0]\n",
    "print(f\"\\nBest Performing Model: {best_model}\")\n",
    "print(\"\\nBest Model Parameters:\")\n",
    "print(modeling.models[best_model].get_params())\n",
    "\n",
    "# Save best model predictions for further analysis\n",
    "best_predictions = modeling.models[best_model].predict(modeling.X_test_scaled)\n",
    "best_probabilities = modeling.models[best_model].predict_proba(modeling.X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate and display confusion matrix\n",
    "cm = confusion_matrix(modeling.y_test, best_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix - {best_model}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Modeling Summary\n",
    "\n",
    "The predictive modeling section has implemented and evaluated three different models:\n",
    "1. Random Forest Classifier\n",
    "2. XGBoost\n",
    "3. LightGBM\n",
    "\n",
    "Key findings from the modeling process:\n",
    "- Model Performance: Comparison of accuracy, precision, recall, and F1-scores\n",
    "- Feature Importance: Identification of key factors influencing booking cancellations\n",
    "- Model Interpretability: SHAP values and partial dependence plots revealing the relationship between features and cancellation probability\n",
    "\n",
    "Next, we'll translate these technical insights into actionable business recommendations in the Business Insights section.\n",
    "\n",
    "## 8. Business Insights\n",
    "\n",
    "In this section, we'll synthesize all our analyses to provide actionable business recommendations:\n",
    "1. Key Findings from Data Analysis\n",
    "2. Revenue Impact Assessment\n",
    "3. Risk Mitigation Strategies\n",
    "4. Actionable Recommendations\n",
    "5. Implementation Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessInsights:\n",
    "    def __init__(self, data, modeling):\n",
    "        self.data = data\n",
    "        self.modeling = modeling\n",
    "        self.insights = {}\n",
    "    \n",
    "    def analyze_key_findings(self):\n",
    "        \"\"\"Analyze and summarize key findings from all analyses.\"\"\"\n",
    "        # Cancellation patterns\n",
    "        cancellation_rate = self.data['is_canceled'].mean() * 100\n",
    "        \n",
    "        # Seasonal patterns\n",
    "        monthly_cancellations = self.data.groupby('arrival_date_month')['is_canceled'].mean() * 100\n",
    "        peak_cancellation_month = monthly_cancellations.idxmax()\n",
    "        \n",
    "        # Lead time analysis\n",
    "        avg_lead_time = self.data.groupby('is_canceled')['lead_time'].mean()\n",
    "        \n",
    "        # Customer segments\n",
    "        segment_cancellations = self.data.groupby('market_segment')['is_canceled'].mean() * 100\n",
    "        high_risk_segment = segment_cancellations.idxmax()\n",
    "        \n",
    "        self.insights['key_findings'] = {\n",
    "            'cancellation_rate': cancellation_rate,\n",
    "            'peak_cancellation_month': peak_cancellation_month,\n",
    "            'lead_time_difference': avg_lead_time[1] - avg_lead_time[0],\n",
    "            'high_risk_segment': high_risk_segment\n",
    "        }\n",
    "        \n",
    "    def calculate_revenue_impact(self):\n",
    "        \"\"\"Calculate revenue impact of cancellations.\"\"\"\n",
    "        # Calculate potential revenue loss\n",
    "        adr_cancelled = self.data[self.data['is_canceled'] == 1]['adr'].sum()\n",
    "        total_cancelled_bookings = self.data['is_canceled'].sum()\n",
    "        \n",
    "        # Calculate revenue recovery potential\n",
    "        best_model = max(self.modeling.results.items(), key=lambda x: x[1]['f1'])[0]\n",
    "        model_accuracy = self.modeling.results[best_model]['accuracy']\n",
    "        potential_savings = adr_cancelled * model_accuracy\n",
    "        \n",
    "        self.insights['revenue_impact'] = {\n",
    "            'total_revenue_loss': adr_cancelled,\n",
    "            'cancelled_bookings': total_cancelled_bookings,\n",
    "            'potential_savings': potential_savings\n",
    "        }\n",
    "    \n",
    "    def generate_recommendations(self):\n",
    "        \"\"\"Generate actionable recommendations based on analysis.\"\"\"\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': self.modeling.feature_names,\n",
    "            'importance': self.modeling.models['random_forest'].feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Generate recommendations based on top features\n",
    "        recommendations = []\n",
    "        for _, row in feature_importance.head().iterrows():\n",
    "            feature = row['feature']\n",
    "            if 'lead_time' in feature.lower():\n",
    "                recommendations.append(\"Implement dynamic pricing based on lead time\")\n",
    "            elif 'market_segment' in feature.lower():\n",
    "                recommendations.append(\"Develop targeted retention strategies for high-risk segments\")\n",
    "            elif 'deposit' in feature.lower():\n",
    "                recommendations.append(\"Optimize deposit policies based on booking characteristics\")\n",
    "            elif 'month' in feature.lower():\n",
    "                recommendations.append(\"Adjust pricing and policies for seasonal patterns\")\n",
    "                \n",
    "        self.insights['recommendations'] = recommendations\n",
    "        \n",
    "    def create_implementation_plan(self):\n",
    "        \"\"\"Create an implementation plan for recommendations.\"\"\"\n",
    "        self.insights['implementation_plan'] = {\n",
    "            'short_term': [\n",
    "                \"Update deposit policies\",\n",
    "                \"Implement email confirmation system\",\n",
    "                \"Train staff on new procedures\"\n",
    "            ],\n",
    "            'medium_term': [\n",
    "                \"Develop dynamic pricing system\",\n",
    "                \"Create customer segment strategies\",\n",
    "                \"Implement automated reminder system\"\n",
    "            ],\n",
    "            'long_term': [\n",
    "                \"Build predictive cancellation system\",\n",
    "                \"Develop loyalty program\",\n",
    "                \"Implement revenue optimization system\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Generate business insights\n",
    "insights = BusinessInsights(data_clean, modeling)\n",
    "insights.analyze_key_findings()\n",
    "insights.calculate_revenue_impact()\n",
    "insights.generate_recommendations()\n",
    "insights.create_implementation_plan()\n",
    "\n",
    "# Display key findings\n",
    "print(\"\\nKey Findings:\")\n",
    "for metric, value in insights.insights['key_findings'].items():\n",
    "    print(f\"{metric.replace('_', ' ').title()}: {value:.2f}\" if isinstance(value, float) else f\"{metric.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\nRevenue Impact:\")\n",
    "for metric, value in insights.insights['revenue_impact'].items():\n",
    "    print(f\"{metric.replace('_', ' ').title()}: ${value:,.2f}\")\n",
    "\n",
    "print(\"\\nTop Recommendations:\")\n",
    "for i, rec in enumerate(insights.insights['recommendations'], 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\nImplementation Plan:\")\n",
    "for phase, actions in insights.insights['implementation_plan'].items():\n",
    "    print(f\"\\n{phase.replace('_', ' ').title()} Actions:\")\n",
    "    for action in actions:\n",
    "        print(f\"- {action}\")\n",
    "\n",
    "# Visualize key insights\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Revenue Impact\n",
    "plt.subplot(2, 2, 1)\n",
    "revenue_data = [insights.insights['revenue_impact']['total_revenue_loss'],\n",
    "                insights.insights['revenue_impact']['potential_savings']]\n",
    "plt.bar(['Total Revenue Loss', 'Potential Savings'], revenue_data)\n",
    "plt.title('Revenue Impact Analysis')\n",
    "plt.ylabel('Amount ($)')\n",
    "\n",
    "# Plot 2: Implementation Timeline\n",
    "plt.subplot(2, 2, 2)\n",
    "timeline_data = [len(phase) for phase in insights.insights['implementation_plan'].values()]\n",
    "plt.bar(['Short Term', 'Medium Term', 'Long Term'], timeline_data)\n",
    "plt.title('Implementation Plan Timeline')\n",
    "plt.ylabel('Number of Actions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Review and Production Readiness\n",
    "\n",
    "### Code Quality and Documentation\n",
    "- Modular, object-oriented design with clear class responsibilities\n",
    "- Comprehensive error handling and logging\n",
    "- Well-documented functions and classes\n",
    "- Consistent coding style and naming conventions\n",
    "\n",
    "### Reproducibility\n",
    "- Environment setup and dependency management\n",
    "- Data versioning and preprocessing pipeline\n",
    "- Modular structure for easy maintenance\n",
    "- Clear execution flow\n",
    "\n",
    "### Production Readiness\n",
    "- Scalable data processing\n",
    "- Robust error handling\n",
    "- Performance optimization\n",
    "- Monitoring capabilities\n",
    "\n",
    "### Future Improvements\n",
    "1. Model Deployment\n",
    "   - API development for model serving\n",
    "   - Monitoring system for model performance\n",
    "   - Regular model retraining pipeline\n",
    "\n",
    "2. Additional Features\n",
    "   - Real-time prediction capabilities\n",
    "   - Integration with booking systems\n",
    "   - Automated reporting system\n",
    "\n",
    "3. Optimization\n",
    "   - Feature selection optimization\n",
    "   - Model performance tuning\n",
    "   - Processing pipeline optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Deployment Steps (Optional)\n",
    "\n",
    "### Model Deployment Pipeline\n",
    "1. Model Serialization\n",
    "   - Save trained models using joblib/pickle\n",
    "   - Version control for model artifacts\n",
    "   - Documentation of model parameters and performance\n",
    "\n",
    "2. API Development\n",
    "   - FastAPI/Flask REST API\n",
    "   - Input validation\n",
    "   - Error handling\n",
    "   - Authentication and rate limiting\n",
    "\n",
    "3. Infrastructure Setup\n",
    "   - Container orchestration (Docker/Kubernetes)\n",
    "   - Load balancing\n",
    "   - Auto-scaling configuration\n",
    "   - Monitoring and logging setup\n",
    "\n",
    "4. CI/CD Pipeline\n",
    "   - Automated testing\n",
    "   - Model validation\n",
    "   - Deployment automation\n",
    "   - Rollback procedures\n",
    "\n",
    "5. Monitoring System\n",
    "   - Model performance metrics\n",
    "   - Data drift detection\n",
    "   - Resource utilization\n",
    "   - Alert system\n",
    "\n",
    "6. Documentation\n",
    "   - API documentation\n",
    "   - Deployment procedures\n",
    "   - Maintenance guides\n",
    "   - Troubleshooting procedures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
